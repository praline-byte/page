---
layout: post
title: 《数据密集型应用系统设计》重点总结
categories: 分布式 
description: 
keywords: 系统设计 分布式 复制 分区 共识 负载
---

![](http://assets.processon.com/chart_image/5fb47cf26376895bf96fdbc6.png)

# 复制
## 复制的作用
1. 提高可用性。即使系统的一部分出现故障，系统也能继续工作
2. 提高读取吞吐量。伸缩可以接受读请求的机器数量
3. 减少延迟。数据与用户在地理上接近

## 复制算法
三种流行的变更复制算法：单领导者，多领导者和无领导者

### 单主复制

数据一致性问题：每一次向数据库的写入操作都需要传播到所有副本上，否则副本就会包含不一样的数据。如何确保所有数据都落在了所有的副本上？

最常见的解决方案被称为**基于领导者的复制（主从复制）**，领导者接收写请求（且只允许领导者接收写请求），将数据同步到追随者。

其中同步操作分为同步，半同步，异步，通常在服务端可以配置。

**设置新从库**
1. 获取主库的一致性快照
2. 将快照同步到从库进行加载
3. 从库连接到主库，根据日志序列号/坐标等标识，追上主库的数据变更

**处理节点宕机的办法**
* 从库宕机：追赶恢复
* 主库宕机：故障切换

**自动故障切换的标准**
1. 确认主库宕机。手段是判断超时
2. 选举新主库。共识问题：见分布式共识
3. 重置请求路由。如果老领导回来了，确保他现在是从库

**故障切换的问题**
1. 如果是异步复制。新主库未收到老主库宕机前最后的写入操作，老主库重新加入集群时，容易产生写入冲突（如mysql自增主键重复）
>解决方案是，丢弃老主库宕机前的写入。但会带来数据丢失的问题。如果数据库同时和外部存储联动，则还可能带来数据不一致的问题
2. 脑裂问题。旧主库重新加入集群时，集群可能认为存在两个主。
>解决方案是，关闭其中一个，但不合理的实现可能会发生两个都被关闭
3. 超时参数如何设置。

### 多主复制

![](/images/posts/DDIA_images/多主复制.png)

多领导者复制的最大问题是可能发生写冲突。

如协同编辑，两个用户同时编辑页面，用户1将页面的标题从A更改为B，并且用户2同时将标题从A更改为C。

**写入冲突**
每个用户的更改已成功应用到其本地主库。但当异步复制时，会发现冲突。

解决方法是：
1. 给每个写入一个唯一的ID（或给每个副本一个ID），挑选最高ID的写入作为胜利者，并丢弃其他写入。如果使用时间戳，这种技术被称为最后写入胜利。虽然这种方法很流行，但是很容易造成数据丢失
2. 以某种方式将这些值合并在一起
3. 用一种可保留所有信息的显式数据结构来记录冲突，并编写解决冲突的应用程序代码（用户提示）

**冲突检测**
1. 最后写入胜利（LWW），挑选“最近”的时间戳（也可以是任期，序号等表示逻辑时序的标识），并丢弃具有较早时间戳的任何写入。有一些情况，如缓存，其中丢失的写入可能是可以接受的。如果丢失数据不可接受，LWW是解决冲突的一个很烂的选择
2. 合并同时写入的值，需要特定的数据结构如CRDT

### 无主复制
无主复制（版本/无顺序保证）

**读修复和反熵**

某个副本故障，版本落后于其他副本时，进行读修复，反熵即不断查找复制副本之间的差异

**法定人数一致性**

法定人数条件 w + r > n
> w: 写入节点数量
>
> r: 读取节点数量
>
> n: 集群节点总数

表示一次写入请求时，只有写成功 w 个节点，才能表示写请求完整，同时一次读取请求时，至少要读取 r 的节点，才能表示读请求完整。

只要满足写入节点数量和读取节点数量之和，比总节点数量大，就说明肯定有节点，即是写入节点又是读取节点，这个节点肯定有最新最完整的数据，以此来保证可用性。

允许系统容忍不可用的节点
* 如果 w < n，如果节点不可用，我们仍然可以处理写入。
* 如果 r < n，如果节点不可用，我们仍然可以处理读取。
* 对于 n = 3，w = 2，r = 2，我们可以容忍一个不可用的节点。 w + r - n = 1
* 对于 n = 4，w = 3，r = 3，我们可以容忍两个不可用的节点。 w + r - n = 2

**法定人数一致性的局限**
1. 如果两个写入同时发生，不清楚哪一个先发生。在这种情况下，唯一安全的解决方案是合并并发写入。如果根据时间戳（最后写入胜利）挑选出一个胜者，则由于时钟偏差，写入可能会丢失。
2. 如果写操作与读操作同时发生，写操作可能仅反映在某些副本上。在这种情况下，不确定读取是返回旧值还是新值。
3. 如果写操作在某些副本上成功，而在其他节点上失败（例如，因为某些节点上的磁盘已满），在小于w个副本上写入成功。所以整体判定写入失败，但整体写入失败并没有在写入成功的副本上回滚。这意味着如果一个写入虽然报告失败，后续的读取仍然可能会读取这次失败写入的值

##  复制延迟
### 读写一致性

![](/images/posts/DDIA_images/读写一致性解决的问题.png)

图中，用户写入后从旧副本中读取了数据，造成读写不一致。需要写后读一致性来防止这种异常。

**基于主从复制的读写一致性方案**
1. 读用户可能已经修改过的内容时，都从主库读。这就要求有一些方法，不用实际查询就可以知道用户是否修改了某些东西，比如用户自身数据从主库读，其他用户从从库读
2. 如果应用中的大部分内容都可能被用户编辑，那这种方法就没用了，因为大部分内容都必须从主库读取（扩容读就没效果了）。
在这种情况下可以使用其他标准来决定是否从主库读取。例如可以跟踪上次更新的时间，在上次更新后的一分钟内，从主库读。同时监控从库的复制延迟，防止对任意比主库滞后超过一分钟的从库发出查询。
3. 客户端可以记住最近一次写入的时间戳，系统需要确保从库为该用户提供任何查询时，该时间戳前的变更都已经传播到了本从库中。如果当前从库不够新，则可以从另一个从库读，或者等待从库追赶上来
时间戳可以是逻辑时间戳如日志序列号LSN，如果是时钟要注意时钟同步

其他复杂问题：如果用户使用不同的设备访问服务，比如桌面和移动端，此时需要考虑元数据(如LSN)需要中心存储，多个数据中心的，需要保证同一个用户在同一个数据中心

### 单调读

![](/images/posts/DDIA_images/单调读问题.png)

问题现象用户先后多次从从库读，先读取延迟小的从库数据，后读取延迟大的从库数据，就会造成时光倒流的现象。

为了防止这种异常，我们需要单调读。

解决方案是一致性哈希使用户只能在一个副本读取，但是如果副本故障重新路由时，还是不可避免此问题。

### 一致前缀读
如果一系列顺序写入，但是落在不同分区的延迟不一致，导致读取时发生顺序错乱。

这是分区/分片数据库中的一个特殊问题。

如果数据库总是以相同的顺序应用写入，则读取总是会看到一致的前缀，所以这种异常不会发生。

但是在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序，当用户从数据库中读取数据时，可能会看到数据库的某些部分处于较旧的状态，而某些处于较新的状态

一种解决方案是，确保任何因果相关的写入都写入相同的分区。

### 监控陈旧度
监控陈旧度。对于可操作性来说，能够量化“最终”是很重要的（最终一致性）。

合理配置的法定人数可以使数据库无需故障切换即可容忍个别节点的故障。也可以容忍个别节点变慢，因为请求不必等待所有n个节点响应——当w或r节点响应时它们可以返回。

对于需要**高可用**、**低延时**、且能够**容忍偶尔读到陈旧值**的应用场景来说，这些特性使无主复制的数据库很有吸引力。


# 分区
在 MongoDB，Elasticsearch 和 Solr Cloud 中被称为分片（shard），在 HBase 中称之为区域（Region），Bigtable 中则是表块（tablet），Cassandra 和 Riak中是虚节点（vnode), Couchbase 中叫做虚桶（vBucket）。

但是分区（partition) 是约定俗成的叫法。

目的：可伸缩性，大数据集可以分布在多个磁盘上，并且查询负载可以分布在多个处理器上。

## 分区与复制

分区通常与复制结合使用，使得每个分区的副本存储在多个节点上。 

这意味着，即使每条记录属于一个分区，它仍然可以存储在多个不同的节点上以获得容错能力。

## 分区的手段
### Key Range 分区
根据键的范围分区，每个分区指定一块连续的键范围（从最小值到最大值），如ABCD。

键的范围不一定均匀分布，因为数据也很可能不均匀分布。

好处是进行范围扫描非常简单，缺点是某些特定的访问模式会导致热点，如按时间分区，请求集中落在最近的时间分区，其他时间分区处于空闲。

需要使用除了时间戳以外的其他东西作为主键的第一个部分，范围查询时，需要给每个主键单独执行范围查询。

### Key Hash 分区
使用散列函数来确定给定键的分区。

丧失范围查询的能力，曾经相邻的密钥现在分散在所有分区中，所以它们之间的顺序就丢失了。

热点倾斜：在极端情况下，所有的读写操作都是针对同一个键的，所有的请求都会被路由到同一个分区。如微博的明星吃瓜事件。

一个简单的方法是在主键的开始或结尾添加一个随机数。只要一个两位数的十进制随机数就可以将主键分散为100种不同的主键,从而存储在不同的分区中。

读取时必须从所有100个主键分布中读取数据并将其合并，同时要决策哪些键需要被分割

## 分片与二级索引
### 基于文档的分区
![](/images/posts/DDIA_images/基于文档的二级索引分区.png)

在这种索引方法中，每个分区是完全独立的：每个分区维护自己的二级索引，仅覆盖该分区中的文档。它不关心存储在其他分区的数据。

文档分区索引也被称为本地索引。

查询时则需要将查询发送到所有分区，并合并所有返回的结果。有尾部放大效应。

MongoDB，Cassandra，Elasticsearch，SolrCloud 都使用文档分区二级索引。

### 基于关键词的分区
![](/images/posts/DDIA_images/基于关键字的二级索引分区.png)
覆盖所有分区数据的全局索引，而不是给每个分区创建自己的本地索引。

但是，我们不能只把这个索引存储在一个节点上，因为它可能会成为瓶颈，违背了分区的目的。

全局索引也必须进行分区，但可以采用与主键不同的分区方式。

关键词分区的全局索引可以使读取更有效率：不需要分散/收集所有分区，客户端只需要向包含关键词的分区发出请求。

全局索引的缺点在于写入速度较慢且较为复杂，因为写入单个文档现在可能会影响索引的多个分区（文档中的每个关键词可能位于不同的分区或者不同的节点上）。

## 平衡策略
### 按节点配置分区
当分区增长到超过配置的大小时，会被分成两个分区，每个分区约占一半的数据。

与之相反，如果大量数据被删除并且分区缩小到某个阈值以下，则可以将其与相邻分区合并。此过程与B树顶层发生的过程类似。

每个分区分配给一个节点，每个节点可以处理多个分区，就像固定数量的分区一样。大型分区拆分后，可以将其中的一半转移到另一个节点，以平衡负载。

动态分区的一个优点是分区数量适应总数据量。如果只有少量的数据，少量的分区就足够了，所以开销很小。如果有大量的数据，每个分区的大小被限制在一个可配置的最大值。

**预分割**

数据集开始时很小，直到达到第一个分区的分割点，所有写入操作都必须由单个节点处理，而其他节点则处于空闲状态。为了解决这个问题，HBase 和 MongoDB 允许在一个空的数据库上配置一组初始分区。

### 按节点比例分区
通过动态分区，分区的数量与数据集的大小成正比，因为拆分和合并过程将每个分区的大小保持在固定的最小值和最大值之间。

另一方面，对于固定数量的分区，每个分区的大小与数据集的大小成正比。在这两种情况下，分区的数量都与节点的数量无关。

每个节点具有固定数量的分区。在这种情况下，每个分区的大小与数据集大小成比例地增长，而节点数量保持不变，但是当增加节点数时，分区将再次变小。

由于较大的数据量通常需要较大数量的节点进行存储，因此这种方法也使每个分区的大小较为稳定。

**再平衡的运维需要手动**

再平衡是一个昂贵的操作，因为它需要重新路由请求并将大量数据从一个节点移动到另一个节点。如果没有做好，这个过程可能会使网络或节点负载过重，降低其他请求的性能。


## 路由
![](/images/posts/DDIA_images/三种不同的路由策略.png)

通常需要类似zk的元数据中心同步路由节点的信息

# 全序广播

要求将消息按照相同的顺序，恰好传递一次，准确传送到所有节点。

如果仔细思考，这相当于进行了几轮共识：在每一轮中，节点提议下一条要发送的消息，然后决定在全序中下一条要发送的消息。

所以，全序广播相当于重复进行多轮共识，Raft和Zab直接实现了全序广播，因为这样做比重复**一次一值（one value a time）**的共识更高效。在Paxos的情况下，这种优化被称为Multi-Paxos

> 原文链接：https:github.com/Vonng/ddia