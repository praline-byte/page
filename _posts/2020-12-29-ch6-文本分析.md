---
layout: post
title: 手摸手搞搜索（六）—— 文本分析
categories: 搜索引擎
description: 
keywords: 搜索引擎 
---

在上一章中我们知道，为了实现**部分匹配**，需要对一段原始文本进行分析，伴随分词，词处理等操作之后，建立倒排索引。

如何把这套机制，在搜索引擎中实现，是本章需要交代的内容。

# 数据解析流程
## 为什么要解析
实际上**将数据放到搜索引擎**需要经过一系列复杂的操作，往往是为了**查得到**和**查的快**而做的操作。

> 比如，希望可以用 "cyj" 拼音缩写来**查找到**"程咬金"
>
> 比如，希望用"咬金"检索"程咬金"也可以**查得快**

明确数据解析的定位，我们把上一章的例子拿过来，顺着拆解一遍数据解析过程中的处理操作。

**“春天来了，万物复苏，又到了动物们繁殖的季节，山林的空气中弥漫着荷尔蒙的气息！”**

## 分词

我们第一步首先将原文拆解成词，让原文内容产生更多的元素。

可能的一种分词结果：**春天**，**复苏**和**繁殖**。

这一步是分词操作，我们称为 Tokenizer。

在搜索中有非常多种的 Tokenizer，比如常见的 standard，whitespace，keyword 等等，每一种都有不同的规则和其适用的场景。

以 ElasticSearch 中的 Tokenizer 举几个例子：

> Ref:https://www.elastic.co/guide/en/elasticsearch/reference/6.0/analysis-tokenizers.html

### Standard Tokenizer

按照 Unicode 切分，支持多语言。

based on the Unicode Text Segmentation algorithm。([Unicode Standard Annex #29](http://unicode.org/reports/tr29/))

```
POST _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

OutPut:
[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]
```

### Letter Tokenizer

遇到不是字母的字符时切分。

PS：对于某些亚洲语言来说，这是很糟糕的，因为亚洲语言中的单词没有空格。

```
POST _analyze
{
  "tokenizer": "letter",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

OutPut:
[ The, QUICK, Brown, Foxes, jumped, over, the, lazy, dog, s, bone ]
```

### Whitespace Tokenizer

遇到空格时切分。

```
POST _analyze
{
  "tokenizer": "whitespace",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

OutPut:
[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone. ]
```

### Keyword Tokenizer

不分词

```
POST _analyze
{
  "tokenizer": "keyword",
  "text": "New York"
}

OutPut:
[ New York ]
```

### Path Hierarchy Tokenizer

遇节点路径分词

默认参数下类似 Edge NGram Tokenizer

```
POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/one/two/three"
}

OutPut:
[ /one, /one/two, /one/two/three ]
```

除了给定的分词器外，我们也很容易自定义自己的分词器，满足不同业务环境下的需求。（见下文中的 Tokenizer 分词接口 ）


## 转换
有了分词结果之后，往往需要进一步处理，比如过滤停用词，转换生成拼音，同义词，转换大小写等等。

以 ElasticSearch 中的 Token Filter 举几个例子：

### Lowercase Token Filter

小写转换器

```
InPut：
{
  "token": [Chen, Yao, Jin]
}

OutPut:
[chen, yao, jin]
```

### Stop Token Filter

停用词转换器

假设停用词为"的"。

```
InPut：
{
  "token": [程咬金, 的, 大斧头]
}

OutPut:
[程咬金，大斧头]
```

### NGram Filter

N-gram 就像一个滑动窗口，在整个单词上移动指定长度的连续字符序列。 

它们对于查询不使用空格或长单词的字段/语言很有用，比如姓名。

以 window-size = [1,2] 为例：
```
InPut：
{
  "token": [程咬金]
}

OutPut:
[程，程咬，咬，咬金，金]
```

### Edge NGram Filter

类似于 NGram，但仅以开头延伸，滑动窗口的起点不会移动。

相比于 NGram 生成的词少，更省空间，但也只能满足固定开头式的查找

以 window-size = [1,2] 为例：
```
InPut：
{
  "token": [程咬金]
}

OutPut:
[程，程咬]
```

实际上还有一种处理过程 CharFilter (字符转换器)，主要用来针对字符级的操作，比如把"&"转换为"and"，比如去除HTML标记等等，篇幅问题不展开了。

# 工程实现

我们在上文介绍到的，文本分析过程中的多种不同的分词器，转换器。

他们用来应对不同的业务场景，是一种具体的实现，既然是这种范式：**一种业务场景对应一种实现**，那必然面对新的业务场景时，也有可能需要新的具体实现。

所以在实现时需要应对扩展与变化，我们在前面文章解释过，处理应对扩展与变化，就是分离关注点，定义合适的接口。

那么文本分析的接口该怎样定义呢？

## 定义顶层接口

Analyzer 定义分析器
```
// Analyzer 定义分析器
// Tokenizer 用于分词
// TokenFilters 用于分词后 Token 的过滤转换等操作
type Analyzer struct {
	Tokenizer    Tokenizer
	TokenFilters []TokenFilter
}
```

Tokenizer 分词接口
```
// Tokenizer 分词接口
// 方法 Tokenize() 表示将输入分词成一组 Token 集合
// 这一步不会破坏原字段信息，也即不会对原信息增删修改，仅用来分词，分词之后的 Token，可以无损拼接为原始字段
// 增删修改的操作，在 TokenFilter 中完成
type Tokenizer interface {
	Tokenize([]byte) TokenStream
}
```

TokenFilter 过滤转换器接口
```
// TokenFilter 过滤转换器接口
// 对一组 Token 进行增删改等操作，返回操作之后的 Token 集合
type TokenFilter interface {
	Filter(TokenStream) TokenStream
}
```

定义执行流
```
func (a *Analyzer) Analyze(input []byte) TokenStream {
    // 分词
	tokens := a.Tokenizer.Tokenize(input)

    // 加工
	if a.TokenFilters != nil {
		for _, tf := range a.TokenFilters {
			tokens = tf.Filter(tokens)
		}
	}
	return tokens
}

```

>暂时用不到字符处理器，所以不在接口和执行流中出现

## 定义工程目录

![](/images/posts/手摸手搞搜索_images/文本分析工程目录.png)

analysis 用于存放文本分析相关总成，顶层接口定义在根目录 analyze.go 中

analyzer 用于定义各种分析执行器，用于演示定义了简单分析器（不分词+nGram）

tokenizer 用于存放分词实现

token_filter 用于存放转换实现

具体实现细节，可以在 [https://github.com/praline-byte/search-engine](https://github.com/praline-byte/search-engine) 获取。

运行测试结果
```
=== RUN   TestSearch
    TestSearch: search_test.go:32: 索引了数据:【程咬金】
    TestSearch: search_test.go:32: 索引了数据:【孙尚香】
    TestSearch: search_test.go:32: 索引了数据:【安琪拉】
    TestSearch: search_test.go:32: 索引了数据:【程龙】
    TestSearch: search_test.go:35: -------
    TestSearch: search_test.go:40: Get 检索:【咬金】
    TestSearch: search_test.go:42: 检索结果:【程咬金】
    TestSearch: search_test.go:43: -------
--- PASS: TestSearch (0.28s)
```

# 本章小结
1. 文本分析过程通常包含字符转换，分词，分词转换三个过程
2. 在工程中实现，顶层目录用于定义接口，子目录用于实现

这样我们的搜索引擎架构演进成了：

![](/images/posts/手摸手搞搜索_images/演进-文本分析.png)
